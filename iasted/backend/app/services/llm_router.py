"""
Service LLM Router Intelligent
Route les requÃªtes vers le LLM optimal selon la complexitÃ© et le coÃ»t
"""

import logging
from typing import Literal, Tuple, Optional, Dict, Any
from enum import Enum
import openai
import anthropic
import google.generativeai as genai

from app.config import settings

logger = logging.getLogger(__name__)


class LLMProvider(str, Enum):
    """Providers LLM disponibles"""
    GEMINI_FLASH = "gemini-flash"
    GPT_4O_MINI = "gpt-4o-mini"
    CLAUDE_HAIKU = "claude-haiku"


class ComplexityLevel(str, Enum):
    """Niveaux de complexitÃ© de requÃªte"""
    SIMPLE = "simple"
    MEDIUM = "medium"
    COMPLEX = "complex"


class LLMRouter:
    """Router intelligent pour dispatcher les requÃªtes LLM"""
    
    def __init__(self):
        """Initialise les clients LLM"""
        genai.configure(api_key=settings.google_ai_api_key)
        self.gemini = genai.GenerativeModel(settings.gemini_model)
        
        self.openai_client = openai.AsyncOpenAI(api_key=settings.openai_api_key)
        
        self.anthropic_client = anthropic.AsyncAnthropic(
            api_key=settings.anthropic_api_key
        )
        
        self.cost_tracker = {
            LLMProvider.GEMINI_FLASH: 0.0,
            LLMProvider.GPT_4O_MINI: 0.0,
            LLMProvider.CLAUDE_HAIKU: 0.0,
        }
        
        logger.info("âœ… LLM Router initialisÃ© (Gemini, OpenAI, Anthropic)")
    
    async def classify_complexity(self, query: str) -> ComplexityLevel:
        """
        Classifie la complexitÃ© d'une requÃªte
        
        Args:
            query: RequÃªte utilisateur
            
        Returns:
            ComplexityLevel: Niveau de complexitÃ© dÃ©tectÃ©
        """
        try:
            prompt = f"""Analyse cette requÃªte et classe sa complexitÃ© en : simple, medium, complex

CritÃ¨res :
- SIMPLE : question factuelle, info basique, salutation
- MEDIUM : analyse simple, comparaison, explication courte
- COMPLEX : analyse profonde, code, raisonnement multi-Ã©tapes, gÃ©nÃ©ration de contenu long

RequÃªte: "{query}"

RÃ©ponds UNIQUEMENT avec un mot : simple, medium ou complex"""
            
            response = await self.gemini.generate_content_async(prompt)
            complexity_str = response.text.strip().lower()
            
            if "simple" in complexity_str:
                return ComplexityLevel.SIMPLE
            elif "complex" in complexity_str:
                return ComplexityLevel.COMPLEX
            else:
                return ComplexityLevel.MEDIUM
                
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur classification complexitÃ©: {e}, dÃ©faut=MEDIUM")
            return ComplexityLevel.MEDIUM
    
    async def route_and_generate(
        self,
        query: str,
        context: Dict[str, Any],
        force_provider: Optional[LLMProvider] = None
    ) -> Tuple[str, LLMProvider, Dict[str, Any]]:
        """
        Route vers le LLM optimal et gÃ©nÃ¨re la rÃ©ponse
        
        Args:
            query: RequÃªte utilisateur
            context: Contexte de conversation (historique, profil, etc.)
            force_provider: Forcer un provider spÃ©cifique (optionnel)
            
        Returns:
            Tuple[str, LLMProvider, Dict]: (rÃ©ponse, provider utilisÃ©, mÃ©tadonnÃ©es)
        """
        try:
            if force_provider:
                provider = force_provider
            else:
                provider = await self._select_provider(query, context)
            
            logger.info(f"ğŸ¤– Routing vers {provider.value} pour: {query[:50]}...")
            
            response, metadata = await self._generate(provider, query, context)
            
            self._track_cost(provider, metadata.get("tokens", 0))
            
            return response, provider, metadata
            
        except Exception as e:
            logger.error(f"âŒ Erreur route_and_generate: {e}", exc_info=True)
            raise
    
    async def _select_provider(
        self,
        query: str,
        context: Dict[str, Any]
    ) -> LLMProvider:
        """
        SÃ©lectionne le provider optimal selon complexitÃ© et coÃ»t
        
        Args:
            query: RequÃªte utilisateur
            context: Contexte
            
        Returns:
            LLMProvider: Provider sÃ©lectionnÃ©
        """
        query_lower = query.lower()
        query_length = len(query.split())
        
        if any(keyword in query_lower for keyword in ["code", "script", "fonction", "class"]):
            return LLMProvider.CLAUDE_HAIKU
        
        if query_length > 500:
            return LLMProvider.CLAUDE_HAIKU
        
        complexity = await self.classify_complexity(query)
        
        if complexity == ComplexityLevel.SIMPLE:
            return LLMProvider.GEMINI_FLASH
        elif complexity == ComplexityLevel.MEDIUM:
            return LLMProvider.GPT_4O_MINI
        else:
            return LLMProvider.CLAUDE_HAIKU
    
    async def _generate(
        self,
        provider: LLMProvider,
        query: str,
        context: Dict[str, Any]
    ) -> Tuple[str, Dict[str, Any]]:
        """
        GÃ©nÃ¨re la rÃ©ponse avec le provider sÃ©lectionnÃ©
        
        Args:
            provider: Provider LLM Ã  utiliser
            query: RequÃªte
            context: Contexte
            
        Returns:
            Tuple[str, Dict]: (rÃ©ponse, mÃ©tadonnÃ©es)
        """
        system_prompt = self._build_system_prompt(context)
        
        if provider == LLMProvider.GEMINI_FLASH:
            return await self._generate_gemini(system_prompt, query)
        elif provider == LLMProvider.GPT_4O_MINI:
            return await self._generate_openai(system_prompt, query)
        elif provider == LLMProvider.CLAUDE_HAIKU:
            return await self._generate_claude(system_prompt, query)
        else:
            raise ValueError(f"Provider inconnu: {provider}")
    
    def _build_system_prompt(self, context: Dict[str, Any]) -> str:
        """Construit le prompt systÃ¨me avec contexte"""
        user_role = context.get("user_role", "user")
        last_turns = context.get("last_turns", [])
        
        prompt = f"""Tu es iAsted, assistant vocal intelligent de la plateforme anti-corruption Ndjobi au Gabon.

Contexte utilisateur :
- RÃ´le : {user_role}
- Permissions : {self._get_role_permissions(user_role)}

Historique rÃ©cent :
{self._format_history(last_turns)}

Directives :
- RÃ©ponds en franÃ§ais naturel, accent gabonais
- Sois concis mais complet
- Cite les sources de donnÃ©es officielles
- Respecte la confidentialitÃ© et le RBAC
- Pour les donnÃ©es sensibles, vÃ©rifie les permissions"""
        
        return prompt
    
    def _get_role_permissions(self, role: str) -> str:
        """Retourne les permissions selon le rÃ´le"""
        permissions_map = {
            "user": "Consultation signalements propres, crÃ©ation rapports",
            "agent": "Consultation rÃ©gionale, attribution cas, analyses",
            "admin": "Consultation nationale, gestion utilisateurs, mÃ©triques",
            "super_admin": "AccÃ¨s complet, administration systÃ¨me"
        }
        return permissions_map.get(role, "Permissions limitÃ©es")
    
    def _format_history(self, turns: list) -> str:
        """Formate l'historique de conversation"""
        if not turns:
            return "Aucun historique"
        
        formatted = []
        for turn in turns[-5:]:
            formatted.append(f"User: {turn.get('user', '')}")
            formatted.append(f"Assistant: {turn.get('assistant', '')}")
        
        return "\n".join(formatted)
    
    async def _generate_gemini(
        self,
        system_prompt: str,
        query: str
    ) -> Tuple[str, Dict[str, Any]]:
        """GÃ©nÃ©ration avec Gemini"""
        try:
            full_prompt = f"{system_prompt}\n\nRequÃªte: {query}"
            
            response = await self.gemini.generate_content_async(full_prompt)
            
            return response.text, {
                "tokens": len(response.text.split()) * 1.3,
                "model": settings.gemini_model
            }
            
        except Exception as e:
            logger.error(f"âŒ Erreur Gemini: {e}")
            raise
    
    async def _generate_openai(
        self,
        system_prompt: str,
        query: str
    ) -> Tuple[str, Dict[str, Any]]:
        """GÃ©nÃ©ration avec OpenAI"""
        try:
            response = await self.openai_client.chat.completions.create(
                model=settings.openai_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                temperature=0.7,
                max_tokens=1024
            )
            
            content = response.choices[0].message.content
            tokens = response.usage.total_tokens
            
            return content, {
                "tokens": tokens,
                "model": settings.openai_model
            }
            
        except Exception as e:
            logger.error(f"âŒ Erreur OpenAI: {e}")
            raise
    
    async def _generate_claude(
        self,
        system_prompt: str,
        query: str
    ) -> Tuple[str, Dict[str, Any]]:
        """GÃ©nÃ©ration avec Claude"""
        try:
            response = await self.anthropic_client.messages.create(
                model=settings.anthropic_model,
                max_tokens=1024,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": query}
                ]
            )
            
            content = response.content[0].text
            tokens = response.usage.input_tokens + response.usage.output_tokens
            
            return content, {
                "tokens": tokens,
                "model": settings.anthropic_model
            }
            
        except Exception as e:
            logger.error(f"âŒ Erreur Claude: {e}")
            raise
    
    def _track_cost(self, provider: LLMProvider, tokens: int):
        """Track les coÃ»ts par provider"""
        cost_per_1m = {
            LLMProvider.GEMINI_FLASH: 0.10,
            LLMProvider.GPT_4O_MINI: 0.15,
            LLMProvider.CLAUDE_HAIKU: 1.00,
        }
        
        cost = (tokens / 1_000_000) * cost_per_1m[provider]
        self.cost_tracker[provider] += cost
    
    def get_cost_stats(self) -> Dict[str, float]:
        """Retourne les statistiques de coÃ»ts"""
        return dict(self.cost_tracker)

