/**
 * iAsted Voice Service - VERSION iOS/MOBILE OPTIMIS√âE
 * 
 * G√®re la reconnaissance vocale (STT) et la synth√®se vocale (TTS)
 * pour l'interaction vocale avec l'assistant pr√©sidentiel.
 * 
 * OPTIMISATIONS iOS/MOBILE:
 * - AudioPool pr√©-initialis√© pour contourner l'autoplay
 * - D√©tection format audio support√© (MP3/AAC/M4A)
 * - MediaRecorder avec fallback WebM/MP4
 * - Web Speech API avec gestion voix iOS
 * - Retry automatique avec d√©lais exponentiels
 */

import { supabase } from '@/integrations/supabase/client';
import { IAstedAudioManager } from './iAstedAudioManager';
import { IAstedSpeechSynthesis } from './iAstedSpeechSynthesis';

export class IAstedVoiceService {
  
  // Configuration vocale
  private static readonly VOICE_CONFIG = {
    language: 'fr-FR', // Fran√ßais
    voiceName: 'fr-FR-DeniseNeural', // Voix f√©minine professionnelle Azure
    // Alternative : 'fr-FR-HenriNeural' pour voix masculine
    rate: 1.0, // Vitesse normale
    pitch: 1.0, // Tonalit√© normale
    volume: 1.0 // Volume maximal
  };

  private static mediaRecorder: MediaRecorder | null = null;
  private static audioChunks: Blob[] = [];
  private static stream: MediaStream | null = null;

  // Expose current mic stream for UI-level audio analysis (read-only)
  static getCurrentStream(): MediaStream | null {
    return this.stream;
  }

  /**
   * Initialiser le syst√®me audio (DOIT √™tre appel√© lors d'une interaction utilisateur)
   */
  static async initializeAudio(): Promise<void> {
    await IAstedAudioManager.initialize();
    await IAstedSpeechSynthesis.initialize();
    console.log('‚úÖ iAsted Voice Service initialis√©');
  }

  /**
   * PARTIE 1 : RECONNAISSANCE VOCALE (Speech-to-Text)
   * 
   * Utilise l'API Web Speech Recognition ou l'API Whisper d'OpenAI
   */

  /**
   * D√©tecter le format MediaRecorder support√©
   */
  private static getSupportedMediaRecorderFormat(): { mimeType: string; extension: string } {
    // iOS supporte audio/mp4
    if (MediaRecorder.isTypeSupported('audio/mp4')) {
      return { mimeType: 'audio/mp4', extension: 'mp4' };
    }
    
    // Android et navigateurs modernes supportent webm
    if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
      return { mimeType: 'audio/webm;codecs=opus', extension: 'webm' };
    }
    
    if (MediaRecorder.isTypeSupported('audio/webm')) {
      return { mimeType: 'audio/webm', extension: 'webm' };
    }
    
    // Fallback
    return { mimeType: '', extension: 'webm' };
  }

  /**
   * D√©marrer l'enregistrement audio avec d√©tection de format
   */
  static async startRecording(): Promise<{ success: boolean; error?: string }> {
    try {
      // Demander la permission d'acc√®s au microphone
      this.stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });

      // D√©tecter le format support√©
      const format = this.getSupportedMediaRecorderFormat();
      console.log('üì± Format MediaRecorder d√©tect√©:', format.mimeType || 'default');

      // Cr√©er le MediaRecorder avec le bon format
      this.mediaRecorder = format.mimeType 
        ? new MediaRecorder(this.stream, { mimeType: format.mimeType })
        : new MediaRecorder(this.stream);

      this.audioChunks = [];

      // Collecter les chunks audio
      this.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          this.audioChunks.push(event.data);
        }
      };

      // D√©marrer l'enregistrement
      this.mediaRecorder.start();

      console.log('üéôÔ∏è Enregistrement d√©marr√©...');
      return { success: true };

    } catch (error: any) {
      console.error('Erreur d√©marrage enregistrement:', error);
      return { 
        success: false, 
        error: error.message || 'Impossible d\'acc√©der au microphone' 
      };
    }
  }

  /**
   * Arr√™ter l'enregistrement et obtenir la transcription
   */
  static async stopRecordingAndTranscribe(): Promise<{
    transcription: string;
    audioBlob: Blob;
    audioUrl: string;
    error?: string;
  } | null> {
    
    return new Promise((resolve) => {
      if (!this.mediaRecorder) {
        console.error('‚ùå MediaRecorder non initialis√©');
        resolve(null);
        return;
      }

      this.mediaRecorder.onstop = async () => {
        try {
          console.log('üéµ Cr√©ation du blob audio...');
          
          // Cr√©er le blob audio
          const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
          const audioUrl = URL.createObjectURL(audioBlob);

          console.log(`üì¶ Blob cr√©√©: ${audioBlob.size} bytes`);

          // Arr√™ter le stream
          if (this.stream) {
            this.stream.getTracks().forEach(track => track.stop());
          }

          if (audioBlob.size < 1000) {
            console.warn('‚ö†Ô∏è Audio trop court, probablement vide');
            resolve({
              transcription: '',
              audioBlob,
              audioUrl,
              error: 'Audio trop court'
            });
            return;
          }

          // Transcrire l'audio avec Deepgram API
          console.log('üîÑ Envoi √† Deepgram pour transcription...');
          const transcription = await this.transcribeAudio(audioBlob);

          console.log('‚úÖ Transcription re√ßue:', transcription);

          resolve({
            transcription,
            audioBlob,
            audioUrl
          });

        } catch (error: any) {
          console.error('‚ùå Erreur transcription:', error);
          resolve({
            transcription: '',
            audioBlob: new Blob([]),
            audioUrl: '',
            error: error.message
          });
        }
      };

      console.log('üõë Arr√™t du MediaRecorder...');
      this.mediaRecorder.stop();
    });
  }

  /**
   * Transcrire un audio avec Deepgram via edge function
   */
  private static async transcribeAudio(audioBlob: Blob): Promise<string> {
    try {
      // Convert blob to base64
      const arrayBuffer = await audioBlob.arrayBuffer();
      const base64Audio = btoa(
        String.fromCharCode(...new Uint8Array(arrayBuffer))
      );

      // Call Deepgram via edge function
      const { data, error } = await supabase.functions.invoke('iasted-stt', {
        body: { audio: base64Audio }
      });

      if (error) throw error;
      if (!data?.text) throw new Error('No transcription returned');

      return data.text;

    } catch (error: any) {
      console.error('Erreur Deepgram:', error);
      
      // Fallback : Web Speech API (moins pr√©cis mais gratuit)
      return this.transcribeWithWebSpeech(audioBlob);
    }
  }

  /**
   * Fallback : Transcription avec Web Speech API
   */
  private static transcribeWithWebSpeech(audioBlob: Blob): Promise<string> {
    // D√©sactiv√© pour √©viter la lecture de l'audio utilisateur en fallback
    return Promise.resolve('');
  }

  /**
   * PARTIE 2 : SYNTH√àSE VOCALE (Text-to-Speech)
   * 
   * Convertit le texte en audio avec voix naturelle
   */

  /**
   * Convertir du texte en audio et le jouer avec ElevenLabs via edge function
   * VERSION iOS OPTIMIS√âE avec AudioManager
   */
  static async speakText(text: string): Promise<{
    success: boolean;
    audioUrl?: string;
    audioBlob?: Blob;
    error?: string;
  }> {
    try {
      // V√©rifier que l'audio est d√©bloqu√©
      if (!IAstedAudioManager.isAudioUnlocked()) {
        console.warn('‚ö†Ô∏è Audio non d√©bloqu√© - tentative fallback Web Speech');
        return this.speakWithWebSpeech(text);
      }

      console.log('üéôÔ∏è Appel ElevenLabs TTS...');

      // Call ElevenLabs via edge function
      const { data, error } = await supabase.functions.invoke('iasted-tts', {
        body: { text, voice: 'XB0fDUnXU5powFXDhCwa' } // Charlotte (FR) par d√©faut
      });

      if (error) throw error;
      if (!data?.audioContent) throw new Error('No audio returned');

      // Convert base64 to blob
      const binaryString = atob(data.audioContent);
      const bytes = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
      }
      
      const audioBlob = new Blob([bytes], { type: 'audio/mpeg' });
      const audioUrl = URL.createObjectURL(audioBlob);

      console.log('üéµ Lecture audio via AudioManager...');

      // Utiliser AudioManager pour la lecture (supporte iOS)
      const playResult = await IAstedAudioManager.playAudioBlob(audioBlob);

      if (playResult.success) {
        console.log('‚úÖ Audio jou√© avec succ√®s');
        return { success: true, audioUrl, audioBlob };
      } else {
        console.warn('‚ö†Ô∏è √âchec AudioManager, fallback Web Speech');
        throw new Error(playResult.error || 'Audio playback failed');
      }

    } catch (error: any) {
      console.error('Erreur ElevenLabs/AudioManager:', error);
      
      // Fallback : Web Speech API (optimis√© pour iOS)
      return this.speakWithWebSpeech(text);
    }
  }

  /**
   * TTS Fallback avec Web Speech API optimis√© pour iOS
   */
  private static async speakWithWebSpeech(text: string): Promise<any> {
    if (!IAstedSpeechSynthesis.isSupported()) {
      return { 
        success: false, 
        error: 'Speech Synthesis non support√©' 
      };
    }

    try {
      console.log('üó£Ô∏è Fallback Web Speech API (iOS optimis√©)');
      
      const result = await IAstedSpeechSynthesis.speak(text, {
        lang: this.VOICE_CONFIG.language,
        rate: this.VOICE_CONFIG.rate,
        pitch: this.VOICE_CONFIG.pitch,
        volume: this.VOICE_CONFIG.volume
      });

      return result;
      
    } catch (error: any) {
      return { 
        success: false, 
        error: error.message || 'Speech synthesis error' 
      };
    }
  }

  /**
   * Arr√™ter la lecture audio en cours
   */
  static stopSpeaking(): void {
    IAstedAudioManager.stop();
    IAstedSpeechSynthesis.stop();
  }

  /**
   * V√©rifier si le micro est disponible
   */
  static async checkMicrophonePermission(): Promise<boolean> {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      stream.getTracks().forEach(track => track.stop());
      return true;
    } catch {
      return false;
    }
  }
}
